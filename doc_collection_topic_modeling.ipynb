{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Document Collections and Quantifying Topic Coverage over Time Series\n",
    "\n",
    "Our time series was every day from 11/12/2020 to 12/11/2020.\n",
    "\n",
    "For the purpose of our topic model, we found that treating every individual word as its own topic was most optimal. Therefore, we quantify the coverage of each given word in our vocabulary on each day in our time series and then select words which have a maximum coverage above our threshold on any given day in our time series. Afterward, the coverage of each topic over time can be compared to the underlying stock price by performing Granger tests. \n",
    "\n",
    "The basis of our algorithm is as follows:\n",
    "\n",
    "    For each of TSLA, PLTR, and NFLX:\n",
    "\n",
    "    1. Build the collection and then build vocabulary for entire collection\n",
    "       1a. Make a set of stop words which we will not include in the vocabulary\n",
    "    2. Build a vocabulary for each day which maps the word to its count on that day\n",
    "        2a. Evaluate the coverage of each term in this vocabulary on that given day\n",
    "    3. For each term in our full document collection vocabulary, make a dictionary mapping\n",
    "    each word a list which is its coverage during each day in our time series.\n",
    "        3a. Only evaluate words which have a coverage over our cutoff (0.001) at some point in the time series\n",
    "    4. Filter for the 200 words which have the highest causality at some point during the time series\n",
    "    5. For each of these words, perform Granger tests to evaluate if there is causal relationship where the \n",
    "    coverage of that given word \"Granger causes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code in this cell parses our input data which we had previously created in tweet_data_tsla.txt,\n",
    "tweet_data_pltr.txt, and tweet_data_nflx.txt into document collection lists with tuples in the form\n",
    "(tweet string, datetime.date())\n",
    "\"\"\"\n",
    "import re\n",
    "import datetime\n",
    "\n",
    "# Helper function for formatting the dates in the doc collection\n",
    "def month_to_num(m):\n",
    "    if m == \"Jan\":\n",
    "        return 1\n",
    "    elif m == \"Feb\":\n",
    "        return 2\n",
    "    elif m == \"Mar\":\n",
    "        return 3\n",
    "    elif m == \"Apr\":\n",
    "        return 4\n",
    "    elif m == \"May\":\n",
    "        return 5\n",
    "    elif m == \"Jun\":\n",
    "        return 6\n",
    "    elif m == \"Jul\":\n",
    "        return 7\n",
    "    elif m == \"Aug\":\n",
    "        return 8\n",
    "    elif m == \"Sep\":\n",
    "        return 9\n",
    "    elif m == \"Oct\":\n",
    "        return 10\n",
    "    elif m == \"Nov\":\n",
    "        return 11\n",
    "    elif m == \"Dec\":\n",
    "        return 12\n",
    "    else:\n",
    "        return -1\n",
    "\n",
    "def create_tweet_collection(infile):\n",
    "    \n",
    "    doc_collection = []\n",
    "    \n",
    "    with open(infile) as fp:\n",
    "            \n",
    "        for tweet in re.findall('--start--(.*?)--end--', fp.read(), re.S):\n",
    "                \n",
    "            split_tweet = tweet.split(\"\\n\")\n",
    "                \n",
    "            unformatted_string_date = split_tweet[1]\n",
    "            # 0 - day of week, 1 - Month as 3-letter, 2 - day, 3 - time (UTC), 4 - timezone add-on (always +0000), 5 - year, 6 - :\n",
    "            split_date = unformatted_string_date.split(\" \")\n",
    "            # a correctly formatted datetime timestamp of the tweet with only the date info\n",
    "            date = datetime.date(int(split_date[5]), month_to_num(split_date[1]), int(split_date[2]))\n",
    "\n",
    "            tweet_string = \"\"\n",
    "\n",
    "            for line in range(2, len(split_tweet)):\n",
    "                tweet_string += split_tweet[line]\n",
    "\n",
    "            dc_tuple = (tweet_string, date)\n",
    "            doc_collection.append(dc_tuple)\n",
    "    \n",
    "    return doc_collection       \n",
    "                \n",
    "tsla_doc_collection = create_tweet_collection(\"tweet_data_tsla.txt\")\n",
    "pltr_doc_collection = create_tweet_collection(\"tweet_data_pltr.txt\")\n",
    "nflx_doc_collection = create_tweet_collection(\"tweet_data_nflx.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code in this cell was used to parse the stopwords.txt file from MP 2.4 and create a set of stop words which we \n",
    "will exclude from our corpus vocabulary to avoid words of low significance.\n",
    "\"\"\"\n",
    "stopwords_file = open(\"stopwords.txt\", \"r\")\n",
    "stopwords_lines = stopwords_file.readlines()\n",
    "\n",
    "stopwords = set()\n",
    "\n",
    "for line in stopwords_lines:\n",
    "    stopwords.add(line.split(\"\\n\")[0])\n",
    "    \n",
    "stopwords_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Helper function which is used to filter out tweets which are not in English\n",
    "\"\"\"\n",
    "def isEnglish(s):\n",
    "    try:\n",
    "        s.encode(encoding='utf-8').decode('ascii')\n",
    "    except UnicodeDecodeError:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The module in this cell is used to create our corpus which will perform all the calculations we need to quantify \n",
    "topic coverage over the time series. \n",
    "\n",
    "Parameters:\n",
    "    - self.documents: array of tuples (date of tweet, array of words in tweet) which represent our document collection\n",
    "      for this corpus\n",
    "    - self.daily_documents: dictionary mapping date to an array of all words used in tweets on that day\n",
    "    - self.vocabulary: array of all unique words from all documents in this collection excluding stop words\n",
    "    - self.daily_vocabulary_dict: nested dictionary; date maps to a dictionary which has the word\n",
    "      as a key and the frequency of that word as its value\n",
    "    - self.daily_word_count: dictionary mapping date to total number of words on that day\n",
    "    - self.daily_term_coverage: nested dict; date maps to a dictionary which has the words coverage on this day;\n",
    "      key is the word and value is total coverage. Coverage = (Term Frequency) / (Count of total words on this day)\n",
    "    - self.documents_path: the path to our list of tuples with tweets and their date\n",
    "    - self.word_coverage_over_time = dictionary mapping a word to an array of tuples containing the date and \n",
    "      the word's coverage on that date\n",
    "    - self.word_max_coverage: dictionary mapping word to its max coverage during time series so we can sort and\n",
    "      filter for the top words which have the highest coverage at any point in our time series.\n",
    "    - self.number_of_documents: the total number of documents in the given collection\n",
    "    - self.vocabulary_size: The total number of unique words in this entire document collection\n",
    "\"\"\"\n",
    "import datetime\n",
    "from statistics import mean\n",
    "\n",
    "class Corpus(object):\n",
    "\n",
    "    def __init__(self, documents_path):\n",
    "        \"\"\"\n",
    "        Initialize parameters and set up document path to get tweets\n",
    "        \"\"\"\n",
    "        self.documents = []\n",
    "        self.daily_documents = dict()\n",
    "        self.vocabulary = []\n",
    "        self.daily_vocabulary_dict = dict()\n",
    "        self.daily_word_count = dict()\n",
    "        self.daily_term_coverage = dict()\n",
    "        self.documents_path = documents_path\n",
    "        self.word_coverage_over_time = dict()\n",
    "        self.word_max_coverage = dict()\n",
    "        self.number_of_documents = 0\n",
    "        self.vocabulary_size = 0\n",
    "\n",
    "    def build_corpus(self):\n",
    "        \"\"\"\n",
    "        Read document, fill in self.documents, a list of list of word\n",
    "        self.documents = [[\"the\", \"day\", \"is\", \"nice\", \"the\", ...], [], []...]\n",
    "        Update self.number_of_documents\n",
    "        \"\"\"\n",
    "        # the doc collection comes in as an array of tuples with the tweet string first and the date next\n",
    "        for tweet, date in self.documents_path:\n",
    "            tweet_words = tweet.split(\" \")\n",
    "            # get all words into lists\n",
    "            self.documents.append((date, tweet_words))\n",
    "            \n",
    "            if date in self.daily_documents:\n",
    "                self.daily_documents[date] += tweet_words\n",
    "            else:\n",
    "                self.daily_documents[date] = tweet_words\n",
    "\n",
    "        self.number_of_documents = len(self.documents)\n",
    "\n",
    "    def build_vocabulary(self):\n",
    "        \"\"\"\n",
    "        Construct a list of unique words in the whole corpus. Put it in self.vocabulary\n",
    "        for example: [\"rain\", \"the\", ...]\n",
    "        Update self.vocabulary_size\n",
    "        \"\"\"\n",
    "        for date, tweet in self.documents:\n",
    "            for word in tweet:\n",
    "                # make sure word is unique\n",
    "                if word not in self.vocabulary and word not in stopwords:\n",
    "                    self.vocabulary.append(word)\n",
    "\n",
    "        self.vocabulary_size = len(self.vocabulary)\n",
    "        \n",
    "    def build_daily_vocab(self):\n",
    "        \"\"\"\n",
    "        Construct a nested dictionary in which each date maps to a dictionary where each word maps to\n",
    "        its total count in tweets on that respective day. Then, compute the coverage of the given word--a \n",
    "        topic in this case--on that day and save that in self.daily_term_coverage.\n",
    "        \"\"\"\n",
    "        for date in self.daily_documents:\n",
    "            \n",
    "            dwc = 0\n",
    "            word_count_dict = dict()\n",
    "            word_coverage_dict = dict()\n",
    "            \n",
    "            for word in self.daily_documents[date]:\n",
    "                dwc += 1\n",
    "                if word in word_count_dict:\n",
    "                    word_count_dict[word] += 1\n",
    "                else:\n",
    "                    word_count_dict[word] = 1\n",
    "        \n",
    "            self.daily_vocabulary_dict[date] = word_count_dict\n",
    "            \n",
    "            self.daily_word_count[date] = dwc\n",
    "            \n",
    "            for w in word_count_dict:\n",
    "                word_coverage_dict[w] = word_count_dict[w] / dwc\n",
    "                \n",
    "            self.daily_term_coverage[date] = word_coverage_dict\n",
    "    \n",
    "    def build_word_cov_over_time(self):\n",
    "        \"\"\"\n",
    "        This function gives us the coverage of every word in the vocabulary over time. To avoid noise, we only \n",
    "        select topics which are heavily covered at some point during our time series.\n",
    "        \"\"\"\n",
    "        for word in self.vocabulary:\n",
    "            if isEnglish(word):\n",
    "                # we will not be able to extract knowledge from tweets in other languages yet\n",
    "                cov_over_time = []\n",
    "\n",
    "                for date in self.daily_term_coverage:\n",
    "                    word_to_cov_map = self.daily_term_coverage[date]\n",
    "                    if word in word_to_cov_map:\n",
    "                        cov_over_time.append(word_to_cov_map[word])\n",
    "                    else:\n",
    "                        cov_over_time.append(0)\n",
    "\n",
    "                # if this word is more than 1/1000 at some point then we mark it as significant\n",
    "                if max(cov_over_time) > 0.001:\n",
    "                    self.word_coverage_over_time[word] = cov_over_time\n",
    "                    self.word_max_coverage[word] = max(cov_over_time)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code in this cell initialized the 3 corpuses we are interested in which are tweets containing $TSLA, $PLTR, or $NFLX.\n",
    "The corpus is initialized, then the appropriate functions are called on each corpus to calculate each given topic/word's \n",
    "coverage over time.\n",
    "\"\"\"\n",
    "tsla_documents_path = tsla_doc_collection\n",
    "pltr_documents_path = pltr_doc_collection\n",
    "nflx_documents_path = nflx_doc_collection\n",
    "\n",
    "tsla_corpus = Corpus(tsla_documents_path)\n",
    "tsla_corpus.build_corpus()\n",
    "tsla_corpus.build_vocabulary()\n",
    "tsla_corpus.build_daily_vocab()\n",
    "tsla_corpus.build_word_cov_over_time()\n",
    "\n",
    "pltr_corpus = Corpus(pltr_documents_path)\n",
    "pltr_corpus.build_corpus()\n",
    "pltr_corpus.build_vocabulary()\n",
    "pltr_corpus.build_daily_vocab()\n",
    "pltr_corpus.build_word_cov_over_time()\n",
    "\n",
    "nflx_corpus = Corpus(nflx_documents_path)\n",
    "nflx_corpus.build_corpus()\n",
    "nflx_corpus.build_vocabulary()\n",
    "nflx_corpus.build_daily_vocab()\n",
    "nflx_corpus.build_word_cov_over_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code in this cell filters for the top 200 most highly covered words/topics in each document collection\n",
    "\"\"\"\n",
    "from operator import itemgetter\n",
    "\n",
    "tsla_top200_words = dict(sorted(tsla_corpus.word_max_coverage.items(), key = itemgetter(1), reverse = True)[:200])\n",
    "pltr_top200_words = dict(sorted(pltr_corpus.word_max_coverage.items(), key = itemgetter(1), reverse = True)[:200])\n",
    "nflx_top200_words = dict(sorted(nflx_corpus.word_max_coverage.items(), key = itemgetter(1), reverse = True)[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The code in this cell converts our term coverage dictionaries into csvs so that they can be cleaned up, the time\n",
    "series data--the underlying stock's price (TSLA, PLTR, or NFLX)--can be added into the csv, and then they can be\n",
    "loaded into R to perform Granger Causality Tests which will tell us which topic's coverage is causaly linked to \n",
    "the respective stock's price.\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "\n",
    "tsla_word_coverage_over_time_df = pd.DataFrame()\n",
    "pltr_word_coverage_over_time_df = pd.DataFrame()\n",
    "nflx_word_coverage_over_time_df = pd.DataFrame()\n",
    "\n",
    "for word in tsla_top200_words:\n",
    "    tsla_word_coverage_over_time_df[word] = tsla_corpus.word_coverage_over_time[word]\n",
    "    \n",
    "for word in pltr_top200_words:\n",
    "    pltr_word_coverage_over_time_df[word] = pltr_corpus.word_coverage_over_time[word]\n",
    "\n",
    "for word in nflx_top200_words:\n",
    "    nflx_word_coverage_over_time_df[word] = nflx_corpus.word_coverage_over_time[word]\n",
    "\n",
    "tsla_word_coverage_over_time_df.to_csv(\"tsla_word_coverage_over_time.csv\")\n",
    "pltr_word_coverage_over_time_df.to_csv(\"pltr_word_coverage_over_time.csv\")\n",
    "nflx_word_coverage_over_time_df.to_csv(\"nflx_word_coverage_over_time.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the next step, which was Granger testing, please see the granger_testing.html or granger_testing.rmd file."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
